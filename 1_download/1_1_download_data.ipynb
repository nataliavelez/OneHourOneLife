{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download public logs\n",
    "Natalia Vélez, November 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import notebook\n",
    "from datetime import datetime\n",
    "import os, csv, requests, urllib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location of One Life logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohol_url = 'http://publicdata.onehouronelife.com/'\n",
    "exclude_keywords = ['..', 'foodLogs', 'curseLog'] # Directories not to download\n",
    "server = 'bigserver2' # server to download from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make local dir to store data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/'\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function: Search through address recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_paths(url, parent=''):\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        response_text = response.text\n",
    "    else:\n",
    "        return response.raise_for_status()\n",
    "    soup = BeautifulSoup(response_text, 'html.parser')\n",
    "    \n",
    "    paths = []\n",
    "    for node in soup.find_all('a'):\n",
    "        node_href = node.get('href')        \n",
    "        # Conditions for recursive search\n",
    "        node_isdir = node_href.endswith('/')\n",
    "        node_keep = not any(kwd in node_href for kwd in exclude_keywords)\n",
    "        \n",
    "        if ((node_isdir) & (node_keep)):\n",
    "            print('crawl —> %s' % parent+node_href)\n",
    "            \n",
    "            # Add folder to local data\n",
    "            local_dir = data_dir + parent + node_href\n",
    "            if not os.path.isdir(local_dir):\n",
    "                print('New local dir: %s' % local_dir)\n",
    "                os.makedirs(local_dir)\n",
    "            \n",
    "            # Recursive search\n",
    "            paths += get_url_paths(url + node_href, parent+node_href)\n",
    "            \n",
    "        elif not node_isdir:\n",
    "            paths.append(url + node.get('href'))\n",
    "            \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find OHOL files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawl —> publicLifeLogData/\n",
      "crawl —> publicLifeLogData/lifeLog/\n",
      "crawl —> publicLifeLogData/lifeLog_bigserver1.onehouronelife.com/\n",
      "crawl —> publicLifeLogData/lifeLog_bigserver2.onehouronelife.com/\n",
      "crawl —> publicLifeLogData/lifeLog_server1.onehouronelife.com/\n",
      "crawl —> publicLifeLogData/lifeLog_server10.onehouronelife.com/\n",
      "crawl —> publicLifeLogData/lifeLog_server11.onehouronelife.com/\n",
      "crawl —> publicLifeLogData/lifeLog_server12.onehouronelife.com/\n",
      "crawl —> publicLifeLogData/lifeLog_server13.onehouronelife.com/\n",
      "crawl —> publicLifeLogData/lifeLog_server14.onehouronelife.com/\n",
      "crawl —> publicLifeLogData/lifeLog_server15.onehouronelife.com/\n",
      "crawl —> publicLifeLogData/lifeLog_server2.onehouronelife.com/\n",
      "crawl —> publicLifeLogData/lifeLog_server3.onehouronelife.com/\n",
      "crawl —> publicLifeLogData/lifeLog_server4.onehouronelife.com/\n",
      "crawl —> publicLifeLogData/lifeLog_server5.onehouronelife.com/\n",
      "crawl —> publicLifeLogData/lifeLog_server6.onehouronelife.com/\n",
      "crawl —> publicLifeLogData/lifeLog_server7.onehouronelife.com/\n",
      "crawl —> publicLifeLogData/lifeLog_server8.onehouronelife.com/\n",
      "crawl —> publicLifeLogData/lifeLog_server9.onehouronelife.com/\n",
      "crawl —> publicMapChangeData/\n",
      "crawl —> publicMapChangeData/bigserver2.onehouronelife.com/\n",
      "26078 files found\n",
      "1624 files in server bigserver2\n"
     ]
    }
   ],
   "source": [
    "ohol_all_paths = get_url_paths(ohol_url)\n",
    "print('%i files found' % len(ohol_all_paths))\n",
    "\n",
    "# Keep only paths in server\n",
    "ohol_paths = [f for f in ohol_all_paths if server in f]\n",
    "print('%i files in server %s' % (len(ohol_paths), server))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data\n",
    "\n",
    "NB: This chunk checks which files are present in the server, but missing from the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1624/1624 files\n"
     ]
    }
   ],
   "source": [
    "data_download = pd.DataFrame({'in': ohol_paths})\n",
    "data_download['out'] = data_download['in'].str.replace(ohol_url, data_dir)\n",
    "data_download['exist'] = data_download['out'].map(os.path.exists)\n",
    "data_download = data_download[~(data_download['exist'])]\n",
    "\n",
    "print('Downloading %i/%i files' % (len(data_download), len(ohol_paths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this download in the download log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = 'download_log.txt'\n",
    "\n",
    "if not os.path.isfile(log_file):\n",
    "    log_header = ['user', 'timestamp', 'files_on_server', 'files_on_local', 'files_downloaded']\n",
    "    with open(log_file, 'wt') as file:\n",
    "        tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "        tsv_writer.writerow(log_header)\n",
    "\n",
    "user = 'nvelez'\n",
    "timestamp = datetime.now().__str__()\n",
    "server = len(ohol_paths)\n",
    "downloaded = len(data_download)\n",
    "local = server-downloaded\n",
    "\n",
    "log_data = [user, timestamp, server, local, downloaded]\n",
    "with open(log_file, 'at') as file:\n",
    "        tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "        tsv_writer.writerow(log_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549d830e10304756a7b46eb90a588c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1624.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_iter = data_download.iterrows()\n",
    "n_iter = data_download.shape[0]\n",
    "\n",
    "for idx, row in notebook.tqdm(data_iter, total=n_iter):\n",
    "    urllib.request.urlretrieve(row['in'], row['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py3]",
   "language": "python",
   "name": "conda-env-.conda-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
